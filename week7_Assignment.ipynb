{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97e31da",
   "metadata": {},
   "source": [
    "### **Week 7 Assignment**\n",
    "#### Sandhya Mainali\n",
    "#### Presidential Graduate School\n",
    "#### PRG 330: Python Programming\n",
    "#### Professor Pant\n",
    "#### Apr 20,2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1411d",
   "metadata": {},
   "source": [
    "### Section A: Algorithm and Data Structure Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abd8b0",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6b880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute-Force Version Performance:\n",
      "\n",
      "Input Size   | Execution Time (s)  \n",
      "-----------------------------------\n",
      "1000         | 0.063160            \n",
      "5000         | 1.143734            \n",
      "10000        | 3.937583            \n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import random\n",
    "\n",
    "# Version 1: Brute-force approach\n",
    "def find_pairs_brute_force(nums, target):\n",
    "    pairs = []\n",
    "    n = len(nums)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if nums[i] + nums[j] == target:\n",
    "                pairs.append((nums[i], nums[j]))\n",
    "    return pairs\n",
    "\n",
    "# Function to test performance with different input sizes\n",
    "def test_brute_force_performance():\n",
    "    input_sizes = [1000, 5000, 10000]\n",
    "    target = 100\n",
    "    results = []\n",
    "\n",
    "    for size in input_sizes:\n",
    "        nums = [random.randint(0, 100) for _ in range(size)]\n",
    "        time_taken = timeit.timeit(\n",
    "            stmt=lambda: find_pairs_brute_force(nums, target),\n",
    "            number=1\n",
    "        )\n",
    "        results.append((size, time_taken))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run and print results\n",
    "def print_results(results):\n",
    "    print(f\"{'Input Size':<12} | {'Execution Time (s)':<20}\")\n",
    "    print(\"-\" * 35)\n",
    "    for size, time_taken in results:\n",
    "        print(f\"{size:<12} | {time_taken:<20.6f}\")\n",
    "\n",
    "# Main driver\n",
    "if __name__ == \"__main__\":\n",
    "    results = test_brute_force_performance()\n",
    "    print(\"Brute-Force Version Performance:\\n\")\n",
    "    print_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60fd208",
   "metadata": {},
   "source": [
    "- O(nÂ²)\n",
    "- The reason for using nested loops involves iterating over succeeding elements for all current elements.\n",
    "- Apart from the result list the algorithm exclusively operates through i, j and other scalar variables.\n",
    "- Extremely poor with big datasets.\n",
    "- Applications that demand real-time performance should avoid the usage of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ba5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optimized Version Performance:\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size   | Execution Time (s)  \n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1000         | 0.000734            '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'10000        | 0.008181            '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'100000       | 0.047230            '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timeit\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "# Version 2: Optimized approach using a set\n",
    "def find_pairs_optimized(nums, target):\n",
    "    seen = set()\n",
    "    pairs = set()\n",
    "\n",
    "    for num in nums:\n",
    "        complement = target - num\n",
    "        if complement in seen:\n",
    "            pairs.add(tuple(sorted((num, complement))))\n",
    "        seen.add(num)\n",
    "\n",
    "    return list(pairs)\n",
    "\n",
    "# Function to test performance with different input sizes\n",
    "def test_optimized_performance():\n",
    "    input_sizes = [1000, 10000, 100000]\n",
    "    target = 100\n",
    "    results = []\n",
    "\n",
    "    for size in input_sizes:\n",
    "        nums = [random.randint(0, 100) for _ in range(size)]\n",
    "        time_taken = timeit.timeit(\n",
    "            stmt=lambda: find_pairs_optimized(nums, target),\n",
    "            number=1\n",
    "        )\n",
    "        results.append((size, time_taken))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to print results in table format\n",
    "def fun(results):\n",
    "    print(f\"{'Input Size':<12} | {'Execution Time (s)':<20}\")\n",
    "    print(\"-\" * 35)\n",
    "    for size, time_taken in results:\n",
    "        display(f\"{size:<12} | {time_taken:<20.6f}\")\n",
    "\n",
    "# Main driver\n",
    "if __name__ == \"__main__\":\n",
    "    results = test_optimized_performance()\n",
    "    display(\"Optimized Version Performance:\\n\")\n",
    "    fun(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5cbd6",
   "metadata": {},
   "source": [
    "- The time complexity is O(n) according to this version which presents the information well.\n",
    "- The program functions proficiently with more than 100,000 elements in its system.\n",
    "- Every element receives one visit.\n",
    "- The operation duration for sets lasts an average of O(1).\n",
    "- Each pair addition to the set takes the same duration as sorting because pair length remains constant at 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd77f64",
   "metadata": {},
   "source": [
    "### Section B: Code Profiling and Memory Management "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1c360",
   "metadata": {},
   "source": [
    "#### Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ae608",
   "metadata": {},
   "source": [
    "### Befor Optimizatio using cprofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe0ad214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'file.txt' with over 1500 words.\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample text block with 1500+ words\n",
    "sample_text = (\n",
    "    \"data science machine learning artificial intelligence \" * 200 +\n",
    "    \"python pandas numpy matplotlib seaborn sklearn \" * 150 +\n",
    "    \"model training validation accuracy precision recall f1-score \" * 100\n",
    ")\n",
    "\n",
    "# Save to 'file.txt'\n",
    "with open(\"file.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"Created 'file.txt' with over 1500 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf4ac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most frequent words:\n",
      "\n",
      "data: 200\n",
      "science: 200\n",
      "machine: 200\n",
      "learning: 200\n",
      "artificial: 200\n",
      "intelligence: 200\n",
      "python: 150\n",
      "pandas: 150\n",
      "numpy: 150\n",
      "matplotlib: 150\n",
      "         1045 function calls (1030 primitive calls) in 0.035 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000    0.034    0.034 {built-in method builtins.exec}\n",
      "      2/1    0.001    0.000    0.034    0.034 <string>:1(<module>)\n",
      "      2/1    0.000    0.000    0.034    0.034 2596923764.py:26(profile_word_frequency)\n",
      "      2/1    0.000    0.000    0.021    0.021 decorator.py:229(fun)\n",
      "        1    0.000    0.000    0.021    0.021 history.py:55(only_when_enabled)\n",
      "        1    0.000    0.000    0.019    0.019 history.py:845(writeout_cache)\n",
      "        1    0.000    0.000    0.018    0.018 history.py:833(_writeout_input_cache)\n",
      "        2    0.014    0.007    0.014    0.007 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        1    0.000    0.000    0.013    0.013 2596923764.py:6(word_frequency)\n",
      "        1    0.000    0.000    0.011    0.011 interactiveshell.py:315(_modified_open)\n",
      "        1    0.011    0.011    0.011    0.011 {built-in method _io.open}\n",
      "        1    0.003    0.003    0.003    0.003 {method 'execute' of 'sqlite3.Connection' objects}\n",
      "        2    0.000    0.000    0.002    0.001 base_events.py:1909(_run_once)\n",
      "        4    0.000    0.000    0.002    0.000 events.py:86(_run)\n",
      "        4    0.000    0.000    0.002    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:209(findall)\n",
      "        2    0.000    0.000    0.001    0.001 zmqstream.py:607(_handle_events)\n",
      "        1    0.000    0.000    0.001    0.001 asyncio.py:612(_handle_select)\n",
      "        1    0.000    0.000    0.001    0.001 asyncio.py:621(_handle_event)\n",
      "        1    0.000    0.000    0.001    0.001 asyncio.py:200(_handle_events)\n",
      "        1    0.001    0.001    0.001    0.001 {method 'findall' of 're.Pattern' objects}\n",
      "        2    0.000    0.000    0.001    0.000 zmqstream.py:648(_handle_recv)\n",
      "        1    0.000    0.000    0.001    0.001 threading.py:323(wait)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:580(_run_callback)\n",
      "      6/5    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:156(_handle_event)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:280(_compile)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:275(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:277(_really_send)\n",
      "        4    0.000    0.000    0.000    0.000 attrsettr.py:42(__getattr__)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:698(send_multipart)\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:740(compile)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:708(__set__)\n",
      "        1    0.000    0.000    0.000    0.000 kernelbase.py:294(poll_control_queue)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3631(set)\n",
      "        1    0.000    0.000    0.000    0.000 decorator.py:199(fix)\n",
      "        4    0.000    0.000    0.000    0.000 attrsettr.py:65(_get_attr_opt)\n",
      "        2    0.000    0.000    0.000    0.000 base_events.py:838(call_soon_threadsafe)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:689(set)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:595(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 ioloop.py:742(_run_callback)\n",
      "        1    0.000    0.000    0.000    0.000 _base.py:537(set_result)\n",
      "        1    0.000    0.000    0.000    0.000 zmqstream.py:718(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 _base.py:337(_invoke_callbacks)\n",
      "        1    0.000    0.000    0.000    0.000 futures.py:398(_call_set_state)\n",
      "       15    0.000    0.000    0.000    0.000 socket.py:621(send)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:669(update)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _collections._count_elements}\n",
      "        2    0.000    0.000    0.000    0.000 windows_events.py:443(select)\n",
      "        2    0.000    0.000    0.000    0.000 proactor_events.py:820(_write_to_self)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:687(_rebuild_io_state)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:573(_code)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:624(write)\n",
      "        1    0.000    0.000    0.000    0.000 proactor_events.py:792(_loop_self_reading)\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:969(parse)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:777(recv_multipart)\n",
      "       14    0.000    0.000    0.000    0.000 enum.py:1551(__or__)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:710(_update_handler)\n",
      "        2    0.000    0.000    0.000    0.000 windows_events.py:761(_poll)\n",
      "        8    0.000    0.000    0.000    0.000 enum.py:1562(__and__)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:718(_validate)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:482(recv)\n",
      "  182/178    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "       66    0.000    0.000    0.000    0.000 enum.py:1544(_get_value)\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:452(_parse_sub)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3268(bind)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3129(_bind)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1512(_notify_trait)\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:814(_call_soon)\n",
      "      2/1    0.000    0.000    0.000    0.000 _compiler.py:37(_compile)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1523(notify_change)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2945(apply_defaults)\n",
      "        1    0.000    0.000    0.000    0.000 traitlets.py:1527(_notify_observers)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:546(_schedule_flush)\n",
      "        4    0.000    0.000    0.000    0.000 events.py:36(__init__)\n",
      "       28    0.000    0.000    0.000    0.000 enum.py:726(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3474(validate)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:258(schedule)\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:512(_parse)\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:511(_compile_info)\n",
      "        6    0.000    0.000    0.000    0.000 typing.py:392(inner)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2892(args)\n",
      "        6    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method nt._path_exists}\n",
      "        1    0.000    0.000    0.000    0.000 asyncio.py:539(_start_select)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:1221(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:714(_register)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:215(_check_mp_mode)\n",
      "        1    0.000    0.000    0.000    0.000 asyncio.py:225(add_callback)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3624(validate_elements)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:1492(__subclasscheck__)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:225(get)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:92(set_result)\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <frozen codecs>:319(decode)\n",
      "        2    0.000    0.000    0.000    0.000 base_events.py:785(call_soon)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'WSARecv' of '_overlapped.Overlapped' objects}\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:394(notify)\n",
      "        5    0.000    0.000    0.000    0.000 threading.py:302(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:216(_compile_charset)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        6    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "       28    0.000    0.000    0.000    0.000 enum.py:1129(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:243(_optimize_charset)\n",
      "        4    0.000    0.000    0.000    0.000 typing.py:1285(__hash__)\n",
      "        5    0.000    0.000    0.000    0.000 threading.py:299(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:727(_cross_validate)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:459(finish_socket_func)\n",
      "        1    0.000    0.000    0.000    0.000 {function _OverlappedFuture.set_result at 0x0000025ABEABE700}\n",
      "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:566(sending)\n",
      "    57/55    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000    0.000    0.000 {method '_acquire_restore' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:734(time)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:97(empty)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:256(get_nowait)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:618(most_common)\n",
      "        1    0.000    0.000    0.000    0.000 history.py:839(_writeout_output_cache)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:55(__init__)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:519(_is_master_process)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:704(_register_with_iocp)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:311(_acquire_restore)\n",
      "       30    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <frozen codecs>:309(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:212(_is_master_process)\n",
      "      2/1    0.000    0.000    0.000    0.000 _parser.py:178(getwidth)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'getresult' of '_overlapped.Overlapped' objects}\n",
      "       10    0.000    0.000    0.000    0.000 _parser.py:168(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:627(clear)\n",
      "        2    0.000    0.000    0.000    0.000 _weakrefset.py:75(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1222(is_alive)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:424(notify_all)\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:231(__init__)\n",
      "       22    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:2304(validate)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen abc>:117(__instancecheck__)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 _parser.py:240(__next)\n",
      "        4    0.000    0.000    0.000    0.000 _parser.py:261(get)\n",
      "        3    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.RLock' objects}\n",
      "       25    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'find' of 'bytearray' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2915(kwargs)\n",
      "        6    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        3    0.000    0.000    0.000    0.000 _parser.py:372(_escape)\n",
      "       24    0.000    0.000    0.000    0.000 typing.py:2182(cast)\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:398(_simple)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:173(qsize)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1155(_wait_for_tstate_lock)\n",
      "        2    0.000    0.000    0.000    0.000 traitlets.py:3486(validate_elements)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:209(_qsize)\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:436(_get_literal_prefix)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:314(_is_owned)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'clear' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _sre.compile}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        2    0.000    0.000    0.000    0.000 _parser.py:293(tell)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "        4    0.000    0.000    0.000    0.000 _parser.py:164(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       10    0.000    0.000    0.000    0.000 inspect.py:2804(kind)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method math.ceil}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        3    0.000    0.000    0.000    0.000 _parser.py:176(append)\n",
      "        2    0.000    0.000    0.000    0.000 _compiler.py:570(isstring)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:137(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 {method '_release_save' of '_thread.RLock' objects}\n",
      "        4    0.000    0.000    0.000    0.000 base_events.py:539(_check_closed)\n",
      "        6    0.000    0.000    0.000    0.000 base_events.py:2004(get_debug)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:322(_consume_expired)\n",
      "        4    0.000    0.000    0.000    0.000 zmqstream.py:562(receiving)\n",
      "        3    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:953(fix_flags)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "        3    0.000    0.000    0.000    0.000 proactor_events.py:883(_process_events)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'fileno' of '_socket.socket' objects}\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:308(_release_save)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'result' of '_asyncio.Future' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'add_done_callback' of '_asyncio.Future' objects}\n",
      "        2    0.000    0.000    0.000    0.000 _parser.py:83(groups)\n",
      "        1    0.000    0.000    0.000    0.000 _compiler.py:467(_get_charset_prefix)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:254(closed)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'cancelled' of '_asyncio.Future' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'done' of '_asyncio.Future' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}\n",
      "        2    0.000    0.000    0.000    0.000 _parser.py:113(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:77(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 _parser.py:256(match)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen codecs>:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _parser.py:172(__setitem__)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2884(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:3085(parameters)\n",
      "        1    0.000    0.000    0.000    0.000 base_events.py:720(is_closed)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:2792(name)\n",
      "        2    0.000    0.000    0.000    0.000 _compiler.py:428(_get_iscased)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:429(_check_closed)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:601(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:59(_set_timeout)\n",
      "      2/0    0.000    0.000    0.000          {built-in method _overlapped.GetQueuedCompletionStatus}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "def word_frequency(file_path):\n",
    "    \"\"\"\n",
    "    Reads a large text file and returns the frequency of each word in descending order.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: List of (word, frequency) tuples.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common()\n",
    "\n",
    "def profile_word_frequency(file_path, top_n=10):\n",
    "    \"\"\"\n",
    "    Profiles the word frequency function and prints the top `n` words.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        top_n (int): Number of top words to display.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = word_frequency(file_path)\n",
    "        print(f\"\\nTop {top_n} most frequent words:\\n\")\n",
    "        for word, count in result[:top_n]:\n",
    "            print(f\"{word}: {count}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "\n",
    "# Set your file path here\n",
    "file_path = 'file.txt'  # Modify this path based on your file's location\n",
    "\n",
    "# Run profiling\n",
    "cProfile.run('profile_word_frequency(file_path)', sort='cumtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a581f0",
   "metadata": {},
   "source": [
    "- Before optimization 1045 function run  in 0.035second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8fc52",
   "metadata": {},
   "source": [
    "### After Optimization using cprofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40504593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most frequent words:\n",
      "\n",
      "data: 200\n",
      "science: 200\n",
      "machine: 200\n",
      "learning: 200\n",
      "artificial: 200\n",
      "intelligence: 200\n",
      "python: 150\n",
      "pandas: 150\n",
      "numpy: 150\n",
      "matplotlib: 150\n",
      "         693 function calls (685 primitive calls) in 0.006 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.002    0.002 3629223846.py:31(profile_word_frequency)\n",
      "        3    0.000    0.000    0.002    0.001 events.py:86(_run)\n",
      "        3    0.000    0.000    0.002    0.001 {method 'run' of '_contextvars.Context' objects}\n",
      "        1    0.000    0.000    0.002    0.002 3629223846.py:6(word_frequency)\n",
      "      2/1    0.000    0.000    0.001    0.001 base_events.py:1909(_run_once)\n",
      "        1    0.000    0.000    0.001    0.001 kernelbase.py:294(poll_control_queue)\n",
      "        1    0.000    0.000    0.001    0.001 _base.py:537(set_result)\n",
      "        1    0.000    0.000    0.001    0.001 _base.py:337(_invoke_callbacks)\n",
      "        1    0.000    0.000    0.001    0.001 futures.py:398(_call_set_state)\n",
      "       11    0.001    0.000    0.001    0.000 socket.py:621(send)\n",
      "        2    0.000    0.000    0.001    0.000 __init__.py:669(update)\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method _collections._count_elements}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "      2/1    0.000    0.000    0.000    0.000 windows_events.py:443(select)\n",
      "      2/1    0.000    0.000    0.000    0.000 windows_events.py:761(_poll)\n",
      "        1    0.000    0.000    0.000    0.000 base_events.py:838(call_soon_threadsafe)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:624(write)\n",
      "  116/112    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 ioloop.py:742(_run_callback)\n",
      "       10    0.000    0.000    0.000    0.000 enum.py:1551(__or__)\n",
      "        1    0.000    0.000    0.000    0.000 zmqstream.py:718(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 zmqstream.py:607(_handle_events)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:687(_rebuild_io_state)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 proactor_events.py:820(_write_to_self)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:1221(__instancecheck__)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:710(_update_handler)\n",
      "        1    0.000    0.000    0.000    0.000 decorator.py:199(fix)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'send' of '_socket.socket' objects}\n",
      "        1    0.000    0.000    0.000    0.000 interactiveshell.py:315(_modified_open)\n",
      "        3    0.000    0.000    0.000    0.000 attrsettr.py:42(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:275(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:277(_really_send)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:546(_schedule_flush)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3268(bind)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:258(schedule)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:3129(_bind)\n",
      "        1    0.000    0.000    0.000    0.000 socket.py:698(send_multipart)\n",
      "       45    0.000    0.000    0.000    0.000 enum.py:1544(_get_value)\n",
      "        1    0.000    0.000    0.000    0.000 zmqstream.py:648(_handle_recv)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:225(get)\n",
      "        1    0.000    0.000    0.000    0.000 proactor_events.py:792(_loop_self_reading)\n",
      "        3    0.000    0.000    0.000    0.000 attrsettr.py:65(_get_attr_opt)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _io.open}\n",
      "        2    0.000    0.000    0.000    0.000 windows_events.py:714(_register)\n",
      "        2    0.000    0.000    0.000    0.000 base_events.py:814(_call_soon)\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:482(recv)\n",
      "        5    0.000    0.000    0.000    0.000 <frozen codecs>:319(decode)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:523(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:595(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 enum.py:1562(__and__)\n",
      "        1    0.000    0.000    0.000    0.000 socket.py:777(recv_multipart)\n",
      "       19    0.000    0.000    0.000    0.000 enum.py:726(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 asyncio.py:225(add_callback)\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 asyncio.py:539(_start_select)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:1492(__subclasscheck__)\n",
      "        2    0.000    0.000    0.000    0.000 windows_events.py:55(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 events.py:36(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 typing.py:392(inner)\n",
      "        1    0.000    0.000    0.000    0.000 zmqstream.py:580(_run_callback)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:676(__get__)\n",
      "        5    0.000    0.000    0.000    0.000 threading.py:302(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:256(get_nowait)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2945(apply_defaults)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'WSARecv' of '_overlapped.Overlapped' objects}\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:394(notify)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "      2/1    0.000    0.000    0.000    0.000 {built-in method _overlapped.GetQueuedCompletionStatus}\n",
      "       19    0.000    0.000    0.000    0.000 enum.py:1129(__new__)\n",
      "        2    0.000    0.000    0.000    0.000 zmqstream.py:566(sending)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:519(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "       34    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method nt._path_exists}\n",
      "        1    0.000    0.000    0.000    0.000 base_events.py:785(call_soon)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <frozen abc>:117(__instancecheck__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:299(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:97(empty)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:215(_check_mp_mode)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2892(args)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:311(_acquire_restore)\n",
      "        4    0.000    0.000    0.000    0.000 traitlets.py:629(get)\n",
      "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "       22    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:212(_is_master_process)\n",
      "        3    0.000    0.000    0.000    0.000 base_events.py:734(time)\n",
      "        2    0.000    0.000    0.000    0.000 typing.py:1285(__hash__)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:627(clear)\n",
      "       25    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:424(notify_all)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1222(is_alive)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2915(kwargs)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:173(qsize)\n",
      "       23    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        1    0.000    0.000    0.000    0.000 windows_events.py:704(_register_with_iocp)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'add_done_callback' of '_asyncio.Future' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method math.ceil}\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:156(_handle_event)\n",
      "       10    0.000    0.000    0.000    0.000 inspect.py:2804(kind)\n",
      "        2    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 <frozen codecs>:309(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 queue.py:209(_qsize)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "        1    0.000    0.000    0.000    0.000 {method '_acquire_restore' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1155(_wait_for_tstate_lock)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:75(__contains__)\n",
      "       19    0.000    0.000    0.000    0.000 3629223846.py:27(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:322(_consume_expired)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:2884(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:137(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        9    0.000    0.000    0.000    0.000 typing.py:2182(cast)\n",
      "        5    0.000    0.000    0.000    0.000 base_events.py:2004(get_debug)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'clear' of 'list' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'result' of '_asyncio.Future' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "        2    0.000    0.000    0.000    0.000 windows_events.py:429(_check_closed)\n",
      "        1    0.000    0.000    0.000    0.000 queues.py:59(_set_timeout)\n",
      "        3    0.000    0.000    0.000    0.000 zmqstream.py:562(receiving)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        2    0.000    0.000    0.000    0.000 proactor_events.py:883(_process_events)\n",
      "        2    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:254(closed)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'fileno' of '_socket.socket' objects}\n",
      "        2    0.000    0.000    0.000    0.000 base_events.py:539(_check_closed)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:2792(name)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'cancelled' of '_asyncio.Future' objects}\n",
      "        1    0.000    0.000    0.000    0.000 base_events.py:720(is_closed)\n",
      "        4    0.000    0.000    0.000    0.000 inspect.py:3085(parameters)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'remove' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:601(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen codecs>:260(__init__)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import os\n",
    "\n",
    "def word_frequency(file_path, top_n=None):\n",
    "    \"\"\"\n",
    "    Reads a large text file and returns the frequency of each word in descending order.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        top_n (int, optional): Return only the top N most frequent words.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: List of (word, frequency) tuples.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "\n",
    "    # Read the file line by line for memory efficiency\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.lower().split()  # Basic whitespace tokenizer\n",
    "            word_counts.update(words)\n",
    "\n",
    "    # Efficient top-N retrieval\n",
    "    if top_n:\n",
    "        return heapq.nlargest(top_n, word_counts.items(), key=lambda x: x[1])\n",
    "    else:\n",
    "        return word_counts.most_common()\n",
    "\n",
    "def profile_word_frequency():\n",
    "    file_path = \"file.txt\"  # Update this path as needed\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    result = word_frequency(file_path, top_n=10)\n",
    "    print(\"\\nTop 10 most frequent words:\\n\")\n",
    "    for word, freq in result:\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "# Run profiling (works in scripts or Jupyter with minor tweaks)\n",
    "if __name__ == \"__main__\":\n",
    "    cProfile.run('profile_word_frequency()', sort='cumtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138db4b4",
   "metadata": {},
   "source": [
    "- After optimization 693 function run in 0.006 second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a444d9b",
   "metadata": {},
   "source": [
    "## Before Optimization memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18fe66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_frequencies(file_path):\n",
    "    print(\"[*] compute_word_frequencies started\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    print(\"[*] File read successfully\")\n",
    "    words = text.split()\n",
    "    frequencies = {}\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        frequencies[word] = frequencies.get(word, 0) + 1\n",
    "    print(\"[*] Word frequency computed\")\n",
    "    return frequencies\n",
    "\n",
    "def run_frequency_analysis():\n",
    "    print(\"[*] Running frequency analysis for memory profiling...\")\n",
    "    compute_word_frequencies(\"file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fb1a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Current working directory: c:\\Users\\sandh\\OneDrive\\Desktop\\python\n",
      "[*] file.txt exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"[*] Current working directory:\", os.getcwd())\n",
    "print(\"[*] file.txt exists:\", os.path.exists(\"file.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623faaf",
   "metadata": {},
   "source": [
    "## After Optimizagion of memory-profiler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca72836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_16924\\3617867365.py\n",
      "[('data', 200), ('science', 200), ('machine', 200), ('learning', 200), ('artificial', 200), ('intelligence', 200), ('python', 150), ('pandas', 150), ('numpy', 150), ('matplotlib', 150)]\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "@profile\n",
    "def word_frequency(file_path, top_n=None):\n",
    "    \"\"\"\n",
    "    Reads a large text file and returns the frequency of each word in descending order.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): The path to the text file to read.\n",
    "    - top_n (int): Limit the results to the top N most frequent words (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples: A list of tuples where each tuple contains a word and its frequency, sorted by frequency in descending order.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "\n",
    "    # Open and read the file line by line to reduce memory usage\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Convert the line to lowercase and split by spaces (words)\n",
    "            words = line.lower().split()  # Simple split by spaces; works efficiently\n",
    "            word_counts.update(words)  # Update counts with words from the line\n",
    "\n",
    "    # If top_n is provided, get the top N most frequent words using heapq for efficiency\n",
    "    if top_n:\n",
    "        # Use heapq to get the top N frequent words in O(n log k) time complexity\n",
    "        sorted_word_counts = heapq.nlargest(top_n, word_counts.items(), key=lambda x: x[1])\n",
    "    else:\n",
    "        # Otherwise, return all words sorted by frequency in descending order\n",
    "        sorted_word_counts = word_counts.most_common()\n",
    "\n",
    "    return sorted_word_counts\n",
    "\n",
    "# Profiling the function using cProfile\n",
    "def profile_word_frequency():\n",
    "    file_path = r'file.txt'  # Ensure the file exists\n",
    "    word_frequencies = word_frequency(file_path, top_n=10)\n",
    "    print(word_frequencies)  # <--- Add this line\n",
    "\n",
    "\n",
    "# Profile the function\n",
    "if __name__ == \"__main__\":\n",
    "    profile_word_frequency()  # This will show memory profiling output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b20039",
   "metadata": {},
   "source": [
    "## Section C: Concurrency and Built-in Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6266220",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a33b2",
   "metadata": {},
   "source": [
    "## version 1: Sequential (requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c99824ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/ -> 200\n",
      "https://www.youtube.com/ -> 200\n",
      "https://www.python.org -> 200\n",
      "https://www.wikipedia.org -> 200\n",
      "https://www.github.com -> 200\n",
      "Sequential time: 2.22 seconds\n"
     ]
    }
   ],
   "source": [
    "### version1\n",
    "import requests\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.youtube.com/\",\n",
    "    \"https://www.python.org\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.github.com\"\n",
    "]\n",
    "\n",
    "def fetch_url(url):\n",
    "    response = requests.get(url)\n",
    "    return url, response.status_code\n",
    "\n",
    "def sequential_download():\n",
    "    start = time.time()\n",
    "    results = [fetch_url(url) for url in urls]\n",
    "    duration = time.time() - start\n",
    "    for url, status in results:\n",
    "        print(f\"{url} -> {status}\")\n",
    "    print(f\"Sequential time: {duration:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sequential_download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57e4c5",
   "metadata": {},
   "source": [
    "### Version 2: Multithreaded (concurrent.futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95068cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/ -> 200\n",
      "https://www.youtube.com/ -> 200\n",
      "https://www.python.org -> 200\n",
      "https://www.wikipedia.org -> 200\n",
      "https://www.github.com -> 200\n",
      "Multithreaded time: 0.77 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.youtube.com/\",\n",
    "    \"https://www.python.org\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.github.com\"\n",
    "]\n",
    "\n",
    "def fetch_url(url):\n",
    "    response = requests.get(url)\n",
    "    return url, response.status_code\n",
    "\n",
    "def threaded_download():\n",
    "    start = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(fetch_url, urls))\n",
    "    duration = time.time() - start\n",
    "    for url, status in results:\n",
    "        print(f\"{url} -> {status}\")\n",
    "    print(f\"Multithreaded time: {duration:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threaded_download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a304f",
   "metadata": {},
   "source": [
    "### Version 3:  Asynchronous (asyncio + aiohttp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6950412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/ -> 200\n",
      "https://www.youtube.com/ -> 200\n",
      "https://www.python.org -> 200\n",
      "https://www.wikipedia.org -> 200\n",
      "https://www.github.com -> 200\n",
      "Async time: 1.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.youtube.com/\",\n",
    "    \"https://www.python.org\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.github.com\"\n",
    "]\n",
    "\n",
    "async def fetch_url(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return url, response.status\n",
    "\n",
    "async def async_download():\n",
    "    start = time.time()\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_url(session, url) for url in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    duration = time.time() - start\n",
    "    for url, status in results:\n",
    "        print(f\"{url} -> {status}\")\n",
    "    print(f\"Async time: {duration:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await async_download()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e9227",
   "metadata": {},
   "source": [
    "#### Use timeit to compare execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1377529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Timing Comparison ---\n",
      "Sequential: https://www.google.com/ -> 200\n",
      "Sequential: https://www.youtube.com/ -> 200\n",
      "Sequential: https://www.python.org -> 200\n",
      "Sequential: https://www.wikipedia.org -> 200\n",
      "Sequential: https://www.github.com -> 200\n",
      "\n",
      "Sequential Time: 2.11 seconds\n",
      "Threaded: https://www.google.com/ -> 200\n",
      "Threaded: https://www.youtube.com/ -> 200\n",
      "Threaded: https://www.python.org -> 200\n",
      "Threaded: https://www.wikipedia.org -> 200\n",
      "Threaded: https://www.github.com -> 200\n",
      "Threaded Time: 0.78 seconds\n",
      "Async: https://www.google.com/ -> 200\n",
      "Async: https://www.youtube.com/ -> 200\n",
      "Async: https://www.python.org -> 200\n",
      "Async: https://www.wikipedia.org -> 200\n",
      "Async: https://www.github.com -> 200\n",
      "Async Time: 0.60 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import timeit\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Fix for running in Jupyter or notebooks\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.youtube.com/\",\n",
    "    \"https://www.python.org\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.github.com\"\n",
    "]\n",
    "\n",
    "# Sequential version\n",
    "def download_sequential():\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        print(f\"Sequential: {url} -> {response.status_code}\")\n",
    "\n",
    "# Multithreaded version\n",
    "def fetch_threaded(url):\n",
    "    response = requests.get(url)\n",
    "    return url, response.status_code\n",
    "\n",
    "def download_threaded():\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(fetch_threaded, urls)\n",
    "        for url, status in results:\n",
    "            print(f\"Threaded: {url} -> {status}\")\n",
    "\n",
    "# Asynchronous version\n",
    "async def fetch_async(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return url, response.status\n",
    "\n",
    "async def download_async():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_async(session, url) for url in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for url, status in results:\n",
    "            print(f\"Async: {url} -> {status}\")\n",
    "\n",
    "# Function to run async in a sync context\n",
    "def download_async_wrapper():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(download_async())\n",
    "\n",
    "# --- Timeit Testing ---\n",
    "print(\"\\n--- Timing Comparison ---\")\n",
    "\n",
    "seq_time = timeit.timeit(download_sequential, number=1)\n",
    "print(f\"\\nSequential Time: {seq_time:.2f} seconds\")\n",
    "\n",
    "threaded_time = timeit.timeit(download_threaded, number=1)\n",
    "print(f\"Threaded Time: {threaded_time:.2f} seconds\")\n",
    "\n",
    "async_time = timeit.timeit(download_async_wrapper, number=1)\n",
    "print(f\"Async Time: {async_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffe99c",
   "metadata": {},
   "source": [
    "## Time and Memory usage Comparison\n",
    "Three version of python script that download HTML content from 5 URLs i.e Sequential, multithreading, & Asynchronous version comparison on the basis of time and memory usage \n",
    "- Sequential Version: The implementation with requests library functions by downloading URLs one by one through a sequence.  The program stops running for each request since the previous request must complete which makes it both the easiest and slowest option available.  The execution process takes lengthiest because each URL analysis occurs in sequential order without sharing memory resources.\n",
    "- Multithreading Version: The ThreadPoolExecutor mechanism enables the multithreaded system to download URLs at the same time.  The process becomes faster in the parallel form when multiple threads simultaneously process requests at the same time.  The high thread operation costs with big URL processing renders it a memory-intensive task.\n",
    "- Asynchronous Version: This version demands minimal both time and memory resources because it integrates the combination of asyncio and aiohttp.  The asynchronous mode allows simultaneous URL processing without delay because it enables I/O operations to execute in parallel.  The execution time remains short because this method avoids generating excessive threads or processes while maintaining minimum memory consumption.\n",
    "\n",
    "**Comparing Timing**:\n",
    "\n",
    "- Execution time with the sequential version becomes the longest because this method proceeds with each request one after another.\n",
    "- The parallel processing of multiple requests accelerates performance in the multithreaded version yet thread management needs slow down execution time compared to asynchronous methods.\n",
    "- The asynchronous version proves to be the fastest solution because it manages I/O-bound operations effectively through non-blocking calls.\n",
    "The asynchronous version outperforms the other resource download methods because it optimizes the execution period and system memory especially in I/O-intensive operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f04aa",
   "metadata": {},
   "source": [
    "##### Use memory_profiler to observe memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3eb0311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Timing and Memory Usage Comparison ---\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "Sequential: https://www.google.com/ -> 200\n",
      "Sequential: https://www.youtube.com/ -> 200\n",
      "Sequential: https://www.python.org -> 200\n",
      "Sequential: https://www.wikipedia.org -> 200\n",
      "Sequential: https://www.github.com -> 200\n",
      "\n",
      "Sequential Time: 2.26 seconds\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "Threaded: https://www.google.com/ -> 200\n",
      "Threaded: https://www.youtube.com/ -> 200\n",
      "Threaded: https://www.python.org -> 200\n",
      "Threaded: https://www.wikipedia.org -> 200\n",
      "Threaded: https://www.github.com -> 200\n",
      "Threaded Time: 0.79 seconds\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "ERROR: Could not find file C:\\Users\\sandh\\AppData\\Local\\Temp\\ipykernel_13672\\4137303280.py\n",
      "Async: https://www.google.com/ -> 200\n",
      "Async: https://www.youtube.com/ -> 200\n",
      "Async: https://www.python.org -> 200\n",
      "Async: https://www.wikipedia.org -> 200\n",
      "Async: https://www.github.com -> 200\n",
      "Async Time: 0.94 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import timeit\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from memory_profiler import profile\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Fix for running in Jupyter or notebooks\n",
    "\n",
    "urls = [\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.youtube.com/\",\n",
    "    \"https://www.python.org\",\n",
    "    \"https://www.wikipedia.org\",\n",
    "    \"https://www.github.com\"\n",
    "]\n",
    "\n",
    "# Sequential version with memory profiling\n",
    "@profile\n",
    "def download_sequential():\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        print(f\"Sequential: {url} -> {response.status_code}\")\n",
    "\n",
    "# Multithreaded version with memory profiling\n",
    "@profile\n",
    "def fetch_threaded(url):\n",
    "    response = requests.get(url)\n",
    "    return url, response.status_code\n",
    "\n",
    "@profile\n",
    "def download_threaded():\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(fetch_threaded, urls)\n",
    "        for url, status in results:\n",
    "            print(f\"Threaded: {url} -> {status}\")\n",
    "\n",
    "# Asynchronous version with memory profiling\n",
    "@profile\n",
    "async def fetch_async(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return url, response.status\n",
    "\n",
    "@profile\n",
    "async def download_async():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_async(session, url) for url in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for url, status in results:\n",
    "            print(f\"Async: {url} -> {status}\")\n",
    "\n",
    "# Function to run async in a sync context with memory profiling\n",
    "@profile\n",
    "def download_async_wrapper():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(download_async())\n",
    "\n",
    "# --- Timeit Testing ---\n",
    "print(\"\\n--- Timing and Memory Usage Comparison ---\")\n",
    "\n",
    "seq_time = timeit.timeit(download_sequential, number=1)\n",
    "print(f\"\\nSequential Time: {seq_time:.2f} seconds\")\n",
    "\n",
    "threaded_time = timeit.timeit(download_threaded, number=1)\n",
    "print(f\"Threaded Time: {threaded_time:.2f} seconds\")\n",
    "\n",
    "async_time = timeit.timeit(download_async_wrapper, number=1)\n",
    "print(f\"Async Time: {async_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c3dff",
   "metadata": {},
   "source": [
    "## Time and Memory Usage by memory-profiler\n",
    "1. Sequential Version\n",
    "The first version operates sequentially by getting files individually until every request completes before it starts another.  This approach consumes limited memory while being easy to operate because it deals with requests one by one.  The blocked request system decreases performance levels in comparison to alternative methods.\n",
    "\n",
    "- Time: This method examines each URL independently which makes it the slowest download process in terms of time duration.\n",
    "\n",
    "- Memory: Minimal resources get allocated since the protocol manages one request while rejecting concurrent task processing.\n",
    "\n",
    "2. MultiThreading Version\n",
    "In the multithreaded version of the program the ThreadPoolExecutor component enables parallel downloading of numerous URLs.  The download process becomes faster when multiple requests operate in parallel threads when compared to the sequential execution method.  The handling of multiple threads for better efficiency results in higher memory demands from managing these threads.\n",
    "\n",
    "- Time: This version requires slightly longer time than asynchronous processing though it is swifter than the sequential version thanks to its capability to process activities concurrently.\n",
    "\n",
    "- Memory: The simultaneous use of memory by individual threads within this version makes it require greater memory than the single-threaded sequential implementation.\n",
    "\n",
    "3. Asynchronous Version\n",
    "The asynchronous version oversees multiple simultaneous requests through asyncio and aiohttp which prevents blocking delays.  Memory utilization stays low thus the software can handle numerous requests effectively.  The asynchronous method operates at maximum speed because it skips the requirement for request completion before beginning new ones.\n",
    "\n",
    "- Time:The asynchronous version manages multiple requests simultaneously which makes it operate at the fastest speed among the three versions.\n",
    "\n",
    "- Memory:This application requires minimal memory because the asynchronous tasks it uses require less memory than thread processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc2576",
   "metadata": {},
   "source": [
    "## **Pros and cons of each version**\n",
    "1. Advantages of Sequential Version:\n",
    "\n",
    "- The application's concurrency features and thread management remain simple and intuitive to implement.\n",
    "\n",
    "- Very little memory is required since the system handles only one request at a time.\n",
    "\n",
    "- Such configuration works best for applications managing a reduced number of URLs and requests  (Sequential Vs Asynchronous Programming in Python â SemFio Networks, n.d.)\n",
    "\n",
    "- Cons:\n",
    "\n",
    "- Åerential request processing becomes slow when dealing with large or multiple URLs and extensive datasets.\n",
    "\n",
    "- The blocking design of the program wastes processing time because it needs to wait until the first request completes before starting the next one.\n",
    "\n",
    "2. Advantages of Multithreaded Version:\n",
    "\n",
    "- The utilization of multiple threads within this version enables faster operation as it can download multiple URLs simultaneously better than the sequential version.\n",
    "\n",
    "- The design works well with activities that contain an average number of requests but no CPU performance constraints (What Are the Pros and Cons of Multithreading?, n.d.)\n",
    "\n",
    "\n",
    "\n",
    "- Cons:\n",
    "\n",
    "- Memory usage increases because each created thread requires individual memory storage.\n",
    "\n",
    "- The management structure of threads consumes additional overhead because it requires multiple resources to create and track threads.\n",
    "\n",
    "- Using this technique is inappropriate for big execution tasks since it can reduce performance when there are numerous threads or insufficient system resources(What Are the Pros and Cons of Multithreading?, n.d.)\n",
    "\n",
    "\n",
    "3. Advantages of the Asynchronous Version:\n",
    "\n",
    "- Such a design delivers maximum speed with its asynchronous functionality that allows concurrent request execution through non-blocking operations.\n",
    "\n",
    "- Asynchronous modules split resource allocation between them while using single threads so they occupy less memory space than threading systems.\n",
    "\n",
    "- The asynchronous version functions best for managing many requests with its efficiency on input/output workloads such as web scraping applications (Sequential Vs Asynchronous Programming in Python â SemFio Networks, n.d.)\n",
    "\n",
    "- Cons:\n",
    "\n",
    "- For beginners the difficulty of understanding and fixing asynchronous programs increases which makes them more complicated to learn.\n",
    "\n",
    "- aiohttp and asyncio provide force code execution through additional libraries which might require extra management and setup according to applications.\n",
    "\n",
    "- The technology provides minimal assistance to CPU-intensive tasks but performs exceptionally well on jobs that depend on I/O operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82096999",
   "metadata": {},
   "source": [
    "### Compare and discuss which approach is most efficient and why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edfcaf",
   "metadata": {},
   "source": [
    "- Asynchronous technique represents the most effective solution for I/O-bound operations including URL downloads because it enables simultaneous processing without blocking operations.  The system executes multiple tasks simultaneously thus reducing execution time since it does not need to wait for previous tasks to complete.  Only one thread operates in asynchronous programming while multithreaded systems requires considerable memory utilization.  A large number of I/O-bound operations can be best managed through this option due to its high scalability and ability to work with many running tasks simultaneously.\n",
    "- The multithreaded technique outperforms sequential processing because it enables many threads to work on I/O tasks at the same time.  The system incurs additional memory overhead due to thread management that becomes less efficient as the number of concurrent operations increases.  The asynchronous programming method offers better time-to-scale ratio than sequencing yet performs better at larger levels of concurrent operations.\n",
    "- The sequential processing procedure provides the least difficulty yet achieves the worst execution efficiency during multiple I/O operations.  Magnifying the number of URLs for download increases the required time due to its single-request approach to handle each request.  The low memory utilization of this method makes it incompatible for parallel request processing since it lacks concurrent execution capabilities.\n",
    "- The most efficient processing method for managing many I/O-bound operations like URL downloads appears to be asynchronous programming followed by multithreaded approaches for balancing concurrency requirements and the sequential approach as the most basic yet resource-efficient solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c0f05",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "- Sequential vs Asynchronous programming in Python â SemFio Networks. (n.d.). https://semfionetworks.com/blog/sequential-vs-asynchronous-programming-in-python/\n",
    "- What are the pros and cons of multithreading? (n.d.). Tech Interview Preparation â System Design, Coding & Behavioral Courses | Design Gurus. https://www.designgurus.io/answers/detail/what-are-the-pros-and-cons-of-multithreading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dd7c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
